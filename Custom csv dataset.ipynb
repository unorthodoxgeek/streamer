{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from https://www.kaggle.com/mkowoods/deep-learning-lstm-for-tweet-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "import pandas as pd\n",
    "import keras\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, classification_report, log_loss\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Flatten\n",
    "from keras.layers import Dropout, Conv1D, GlobalMaxPool1D, GRU, GlobalAvgPool1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#training constants\n",
    "MAX_SEQ_LEN = 25 #this is based on a quick analysis of the len of sequences train['text'].apply(lambda x : len(x.split(' '))).quantile(0.95)\n",
    "DEFAULT_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, we'll need to load the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 6)\n",
      "(4800, 6)\n"
     ]
    }
   ],
   "source": [
    "LABEL_COLUMN = 'sentiment'\n",
    "LABELS = [0, 4]\n",
    "\n",
    "train_file_path='./models/training_data/training.1600000.processed.noemoticon.csv'\n",
    "data = pd.read_csv(train_file_path)\n",
    "train, test = train_test_split(data, random_state = 42, train_size=0.03, test_size=0.003)\n",
    "print(train.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we need to encode the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 68697\n",
      "Max Token Index: 68697 \n",
      "\n",
      "Sample Tweet Before Processing: @rahulsood When's the update to the Envy133 coming? I want ont but know that it's trailing edge speed w/o trailing edge price now \n",
      "Sample Tweet After Processing: [\"rahulsood When's the update to the Envy133 coming I want ont but know that it's trailing edge speed w o trailing edge price now\"] \n",
      "\n",
      "What the model will interpret: [0, 19900, 19901, 3, 637, 1, 3, 19902, 274, 2, 73, 12738, 21, 54, 18, 58, 12739, 3449, 3084, 285, 595, 12739, 3449, 2006, 30]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text, mapping):\n",
    "    replace_white_space = [\"\\n\"]\n",
    "    for s in replace_white_space:\n",
    "        text = text.replace(s, \" \")\n",
    "    replace_punctuation = [\"’\", \"‘\", \"´\", \"`\", \"\\'\", r\"\\'\"]\n",
    "    for s in replace_punctuation:\n",
    "        text = text.replace(s, \"'\")\n",
    "    \n",
    "    # Random note: removing the URLs slightly degraded performance, it's possible the model learned that certain URLs were positive/negative\n",
    "    # And was able to extrapolate that to retweets. Could also explain why re-training the Embeddings improves performance.\n",
    "    # remove twitter url's\n",
    "    #     text = re.sub(r\"http[s]?://t.co/[A-Za-z0-9]*\",\"TWITTERURL\",text)\n",
    "    mapped_string = []\n",
    "    for t in text.split(\" \"):\n",
    "        if t in mapping:\n",
    "            mapped_string.append(mapping[t])\n",
    "        elif t.lower() in mapping:\n",
    "            mapped_string.append(mapping[t.lower()])\n",
    "        else:\n",
    "            mapped_string.append(t)\n",
    "    return ' '.join(mapped_string)\n",
    "\n",
    "CONTRACTION_MAPPING = {\n",
    "    \"don't\": \"do not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "}\n",
    "\n",
    "# Get tweets from Data frame and convert to list of \"texts\" scrubbing based on clean_text function\n",
    "# CONTRACTION_MAPPING is a map of common contractions(e.g don't => do not)\n",
    "train_text_vec = [clean_text(text, CONTRACTION_MAPPING) for text in train['text'].values]\n",
    "test_text_vec = [clean_text(text, CONTRACTION_MAPPING) for text in test['text'].values]\n",
    "\n",
    "\n",
    "# tokenize the sentences\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(train_text_vec)\n",
    "train_text_vec = tokenizer.texts_to_sequences(train_text_vec)\n",
    "test_text_vec = tokenizer.texts_to_sequences(test_text_vec)\n",
    "\n",
    "# pad the sequences\n",
    "train_text_vec = pad_sequences(train_text_vec, maxlen=MAX_SEQ_LEN)\n",
    "test_text_vec = pad_sequences(test_text_vec, maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "print('Number of Tokens:', len(tokenizer.word_index))\n",
    "print(\"Max Token Index:\", train_text_vec.max(), \"\\n\")\n",
    "\n",
    "print('Sample Tweet Before Processing:', train[\"text\"].values[0])\n",
    "print('Sample Tweet After Processing:', tokenizer.sequences_to_texts([train_text_vec[0]]), '\\n')\n",
    "\n",
    "print('What the model will interpret:', train_text_vec[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Classes: Counter({4: 24025, 0: 23975})\n",
      "[1.00104275 0.99895942]\n",
      "Dominant Class:  4\n",
      "Baseline Accuracy Dominant Class 0.5020833333333333\n",
      "F1 Score: 0.3310210941121928\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encode Y values:\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train = encoder.fit_transform(train['sentiment'].values)\n",
    "y_train = to_categorical(y_train) \n",
    "\n",
    "y_test = encoder.fit_transform(test['sentiment'].values)\n",
    "y_test = to_categorical(y_test) \n",
    "\n",
    "# get an idea of the distribution of the text values\n",
    "from collections import Counter\n",
    "ctr = Counter(train['sentiment'].values)\n",
    "print('Distribution of Classes:', ctr)\n",
    "\n",
    "# get class weights for the training data, this will be used data\n",
    "y_train_int = np.argmax(y_train,axis=1)\n",
    "cws = class_weight.compute_class_weight('balanced', np.unique(y_train_int), y_train_int)\n",
    "print(cws)\n",
    "\n",
    "print('Dominant Class: ', ctr.most_common(n = 1)[0][0])\n",
    "print('Baseline Accuracy Dominant Class', (ctr.most_common(n = 1)[0][0] == test['sentiment'].values).mean())\n",
    "\n",
    "preds = np.zeros_like(y_test)\n",
    "preds[:, 0] = 1\n",
    "preds[0] = 1 #done to suppress warning from numpy for f1 score\n",
    "print('F1 Score:', f1_score(y_test, preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba, average = None):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(100)]:\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold, average=average)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "\n",
    "\n",
    "def train(model, \n",
    "          X_train, y_train, X_test, y_test, \n",
    "          checkpoint_path='model.hdf5', \n",
    "          epochs = 25, \n",
    "          batch_size = DEFAULT_BATCH_SIZE, \n",
    "          class_weights = None, \n",
    "          fit_verbose=2,\n",
    "          print_summary = True\n",
    "         ):\n",
    "    m = model()\n",
    "    if print_summary:\n",
    "        print(m.summary())\n",
    "    m.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        #this is bad practice using test data for validation, in a real case would use a seperate validation set\n",
    "        validation_data=(X_test, y_test),  \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        class_weight=class_weights,\n",
    "         #saves the most accurate model, usually you would save the one with the lowest loss\n",
    "        callbacks= [\n",
    "            ModelCheckpoint(checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "            EarlyStopping(patience = 2)\n",
    "        ],\n",
    "        verbose=fit_verbose\n",
    "    ) \n",
    "    print(\"\\n\\n****************************\\n\\n\")\n",
    "    #print('Loading Best Model...')\n",
    "    #m.load_weights(checkpoint_path)\n",
    "    predictions = m.predict(X_test, verbose=1)\n",
    "    print('Validation Loss:', log_loss(y_test, predictions))\n",
    "    print('Test Accuracy', (predictions.argmax(axis = 1) == y_test.argmax(axis = 1)).mean())\n",
    "    print('F1 Score:', f1_score(y_test.argmax(axis = 1), predictions.argmax(axis = 1), average='weighted'))\n",
    "    return m #returns best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 128)           12682240  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 12,822,210\n",
      "Trainable params: 12,822,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/.virtualenvs/pulsar/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 8000 samples\n",
      "Epoch 1/5\n",
      " - 179s - loss: 0.4995 - accuracy: 0.7522 - val_loss: 0.4617 - val_accuracy: 0.7828\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/.virtualenvs/pulsar/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 211s - loss: 0.3080 - accuracy: 0.8679 - val_loss: 0.5105 - val_accuracy: 0.7660\n",
      "Epoch 3/5\n",
      " - 188s - loss: 0.1571 - accuracy: 0.9378 - val_loss: 0.6503 - val_accuracy: 0.7588\n",
      "Epoch 4/5\n",
      " - 182s - loss: 0.0874 - accuracy: 0.9662 - val_loss: 0.8673 - val_accuracy: 0.7452\n",
      "Epoch 5/5\n",
      " - 166s - loss: 0.0552 - accuracy: 0.9790 - val_loss: 1.0593 - val_accuracy: 0.7355\n",
      "\n",
      "\n",
      "****************************\n",
      "\n",
      "\n",
      "8000/8000 [==============================] - 2s 233us/step\n",
      "Validation Loss: 1.059322223015625\n",
      "Test Accuracy 0.7355\n",
      "F1 Score: 0.7354722744580906\n"
     ]
    }
   ],
   "source": [
    "# First model version\n",
    "def model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "m1 = train(model_1, \n",
    "           train_text_vec,\n",
    "           y_train,\n",
    "           test_text_vec,\n",
    "           y_test,\n",
    "           epochs=5,\n",
    "           checkpoint_path='/Users/tom/tikal/streamer/checkpoints/model_1.h5',\n",
    "           class_weights=cws\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 25, 128)           12682240  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 25, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 12,961,986\n",
      "Trainable params: 12,961,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/.virtualenvs/pulsar/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 8000 samples\n",
      "Epoch 1/25\n",
      " - 139s - loss: 0.5138 - accuracy: 0.7425 - val_loss: 0.4594 - val_accuracy: 0.7799\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/.virtualenvs/pulsar/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 423s - loss: 0.3662 - accuracy: 0.8397 - val_loss: 0.4686 - val_accuracy: 0.7759\n",
      "Epoch 3/25\n",
      " - 141s - loss: 0.2415 - accuracy: 0.9005 - val_loss: 0.5962 - val_accuracy: 0.7641\n",
      "Epoch 4/25\n",
      " - 161s - loss: 0.1551 - accuracy: 0.9406 - val_loss: 0.6692 - val_accuracy: 0.7499\n",
      "Epoch 5/25\n",
      " - 147s - loss: 0.1105 - accuracy: 0.9574 - val_loss: 0.8145 - val_accuracy: 0.7469\n",
      "Epoch 6/25\n",
      " - 165s - loss: 0.0847 - accuracy: 0.9681 - val_loss: 0.8511 - val_accuracy: 0.7454\n",
      "Epoch 7/25\n",
      " - 158s - loss: 0.0687 - accuracy: 0.9739 - val_loss: 1.0575 - val_accuracy: 0.7467\n",
      "Epoch 8/25\n",
      " - 143s - loss: 0.0568 - accuracy: 0.9785 - val_loss: 1.1072 - val_accuracy: 0.7372\n",
      "Epoch 9/25\n",
      " - 143s - loss: 0.0498 - accuracy: 0.9813 - val_loss: 1.1196 - val_accuracy: 0.7439\n",
      "Epoch 10/25\n",
      " - 143s - loss: 0.0428 - accuracy: 0.9841 - val_loss: 1.1907 - val_accuracy: 0.7421\n",
      "Epoch 11/25\n",
      " - 146s - loss: 0.0378 - accuracy: 0.9857 - val_loss: 1.2147 - val_accuracy: 0.7383\n",
      "Epoch 12/25\n",
      " - 3063s - loss: 0.0333 - accuracy: 0.9876 - val_loss: 1.3297 - val_accuracy: 0.7402\n",
      "Epoch 13/25\n",
      " - 142s - loss: 0.0306 - accuracy: 0.9885 - val_loss: 1.2336 - val_accuracy: 0.7376\n",
      "Epoch 14/25\n",
      " - 140s - loss: 0.0283 - accuracy: 0.9894 - val_loss: 1.2638 - val_accuracy: 0.7386\n",
      "Epoch 15/25\n",
      " - 140s - loss: 0.0258 - accuracy: 0.9903 - val_loss: 1.4268 - val_accuracy: 0.7312\n",
      "Epoch 16/25\n",
      " - 140s - loss: 0.0231 - accuracy: 0.9912 - val_loss: 1.6312 - val_accuracy: 0.7321\n",
      "Epoch 17/25\n",
      " - 142s - loss: 0.0209 - accuracy: 0.9924 - val_loss: 1.6122 - val_accuracy: 0.7334\n",
      "Epoch 18/25\n",
      " - 140s - loss: 0.0188 - accuracy: 0.9929 - val_loss: 1.5345 - val_accuracy: 0.7303\n",
      "Epoch 19/25\n",
      " - 141s - loss: 0.0189 - accuracy: 0.9929 - val_loss: 1.6500 - val_accuracy: 0.7314\n",
      "Epoch 20/25\n",
      " - 394s - loss: 0.0167 - accuracy: 0.9940 - val_loss: 1.6652 - val_accuracy: 0.7295\n",
      "Epoch 21/25\n",
      " - 149s - loss: 0.0154 - accuracy: 0.9943 - val_loss: 1.7264 - val_accuracy: 0.7327\n",
      "Epoch 22/25\n",
      " - 140s - loss: 0.0144 - accuracy: 0.9949 - val_loss: 1.7378 - val_accuracy: 0.7322\n",
      "Epoch 23/25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cab9549c84c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m            \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_1b.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m            \u001b[0mclass_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m            \u001b[0mprint_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m           )\n",
      "\u001b[0;32m<ipython-input-5-7bf6281903ba>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X_train, y_train, X_test, y_test, checkpoint_path, epochs, batch_size, class_weights, fit_verbose, print_summary)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m#EarlyStopping(patience = 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         ],\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_verbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     ) \n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n****************************\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pulsar/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.virtualenvs/pulsar/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pulsar/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pulsar/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pulsar/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pulsar/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pulsar/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.virtualenvs/pulsar/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_1b():\n",
    "    \"\"\"\n",
    "    Using a Bidiretional LSTM. \n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "m1b = train(model_1b, \n",
    "           train_text_vec,\n",
    "           y_train,\n",
    "           test_text_vec,\n",
    "           y_test,\n",
    "           checkpoint_path='model_1b.h5',\n",
    "           class_weights=cws,\n",
    "           print_summary = True\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 25, 128)           12682240  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 25, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 25, 256)           263168    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 22, 64)            65600     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,015,298\n",
      "Trainable params: 13,015,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/.virtualenvs/pulsar/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 8000 samples\n",
      "Epoch 1/25\n",
      " - 151s - loss: 0.5042 - accuracy: 0.7488 - val_loss: 0.4566 - val_accuracy: 0.7824\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/.virtualenvs/pulsar/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 153s - loss: 0.3393 - accuracy: 0.8525 - val_loss: 0.4940 - val_accuracy: 0.7714\n",
      "Epoch 3/25\n",
      " - 158s - loss: 0.1980 - accuracy: 0.9210 - val_loss: 0.6092 - val_accuracy: 0.7655\n",
      "\n",
      "\n",
      "****************************\n",
      "\n",
      "\n",
      "8000/8000 [==============================] - 3s 325us/step\n",
      "Validation Loss: 0.6091855999883811\n",
      "Test Accuracy 0.7655\n",
      "F1 Score: 0.7654859730140383\n"
     ]
    }
   ],
   "source": [
    "def model_1c():\n",
    "    \"\"\"\n",
    "    Adding dropout to reduce overfitting using a bidiretional LSTM\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)))\n",
    "    model.add(Conv1D(64, 4))\n",
    "#     model.add(Flatten())\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "m1c = train(model_1c, \n",
    "           train_text_vec,\n",
    "           y_train,\n",
    "           test_text_vec,\n",
    "           y_test,\n",
    "           checkpoint_path='model_1c.h5',\n",
    "           class_weights=cws,\n",
    "           print_summary = True\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 128)           8793344   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 25, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 21, 64)            41024     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 19, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 18, 64)            8256      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 8,859,266\n",
      "Trainable params: 8,859,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/.virtualenvs/pulsar/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 4800 samples\n",
      "Epoch 1/25\n",
      " - 42s - loss: 0.5413 - accuracy: 0.7178 - val_loss: 0.4866 - val_accuracy: 0.7640\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/.virtualenvs/pulsar/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 41s - loss: 0.3567 - accuracy: 0.8436 - val_loss: 0.5210 - val_accuracy: 0.7504\n",
      "Epoch 3/25\n",
      " - 41s - loss: 0.1785 - accuracy: 0.9301 - val_loss: 0.7180 - val_accuracy: 0.7277\n",
      "\n",
      "\n",
      "****************************\n",
      "\n",
      "\n",
      "4800/4800 [==============================] - 0s 58us/step\n",
      "Validation Loss: 0.7179547698535527\n",
      "Test Accuracy 0.7277083333333333\n",
      "F1 Score: 0.7265767092106544\n"
     ]
    }
   ],
   "source": [
    "def model_1d():\n",
    "    \"\"\"\n",
    "    Just for fun below is a model only using covolutions. This is pretty good and also trains very quickly(and predictions would also likely be fast) compared to the LSTM...\n",
    "    It's equivalent to using an n-gram based approach.\n",
    "    Usually in practice you would use a more complex architecture with multiple parallel convolutions that are combined before pooling(and usually both max and avg).\n",
    "    Pure Convolutional NLP is definitely a solution worth exploring further.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(Conv1D(64, 5))\n",
    "    model.add(Conv1D(64, 3))\n",
    "    model.add(Conv1D(64, 2))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "m1d = train(model_1d, \n",
    "           train_text_vec,\n",
    "           y_train,\n",
    "           test_text_vec,\n",
    "           y_test,\n",
    "           checkpoint_path='model_1d.h5',\n",
    "           class_weights=cws,\n",
    "           print_summary = True\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text):\n",
    "    text_vector = [clean_text(text, CONTRACTION_MAPPING)]\n",
    "    text_vector = tokenizer.texts_to_sequences(text_vector)\n",
    "    text_vector = pad_sequences(text_vector, maxlen=MAX_SEQ_LEN)\n",
    "    prediction = model.predict(text_vector)\n",
    "    arr_prediction = prediction.tolist()[0]\n",
    "    max_prediction = max(arr_prediction)\n",
    "    index = arr_prediction.index(max_prediction)\n",
    "    if max_prediction > 0.75:\n",
    "        if index == 0:\n",
    "            return (\"Negative\", max_prediction)\n",
    "        return (\"Positive\", max_prediction)\n",
    "    return (\"Unknown\", max_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Positive', 0.9623548984527588)\n",
      "('Negative', 0.9230782389640808)\n",
      "('Positive', 0.8561643958091736)\n"
     ]
    }
   ],
   "source": [
    "#positive\n",
    "prediction = predict(m1d, \"@siyab it has been long I have seen you on twitter... Your avatar rocks as always \")\n",
    "print(prediction)\n",
    "\n",
    "#negative\n",
    "prediction = predict(m1d, \"It's sunburns like these that make me hate being a red head. I'd trade it for the ability to tan instead of burn any day...\")\n",
    "print(prediction)\n",
    "\n",
    "prediction = predict(m1d, \"The need for independent journalism has never been greater. Become a Guardian supporter: http://gu.com/supporter/guardiannews\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1d.save(\"twitter_sentiment.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1dloaded = tf.keras.models.load_model('twitter_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(m1dloaded, \"The need for independent journalism has never been greater. Become a Guardian supporter: http://gu.com/supporter/guardiannews\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "import io\n",
    "import json\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "#load tokenizer\n",
    "with open('tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizerr = tokenizer_from_json(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting trained model to models/sentiment_analyzer/model.pd\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.saved_model' has no attribute 'builder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-6dae03c4504d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexport_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'models/sentiment_analyzer/model.pd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exporting trained model to'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedModelBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m builder.add_meta_graph_and_variables(\n\u001b[1;32m      6\u001b[0m       \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_constants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSERVING\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.saved_model' has no attribute 'builder'"
     ]
    }
   ],
   "source": [
    "model = m1d\n",
    "export_path = 'models/sentiment_analyzer/model.pd'\n",
    "print('Exporting trained model to', export_path)\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "builder.add_meta_graph_and_variables(\n",
    "      sess, [tf.saved_model.tag_constants.SERVING],\n",
    "      signature_def_map={\n",
    "           'predict_images':\n",
    "               prediction_signature,\n",
    "           signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
    "               classification_signature,\n",
    "      },\n",
    "      main_op=tf.tables_initializer())\n",
    "builder.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
